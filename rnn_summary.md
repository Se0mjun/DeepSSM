# π” Recurrent Neural Network (RNN) μ •λ¦¬

## π“ μ •μ
- μ‹ν€€μ¤(Sequence) λ°μ΄ν„°λ¥Ό μ²λ¦¬ν•λ” **μν™ κµ¬μ΅°μ μ‹ κ²½λ§**
- μ΄μ „ μ‹μ μ hidden stateλ¥Ό μ‚¬μ©ν•μ—¬ **μ‹κ°„μ μΌλ΅ μ—°μ†λ λ°μ΄ν„°μ ν¨ν„΄μ„ ν•™μµ**

## π§  κΈ°λ³Έ μμ‹

$$
\begin{aligned}
h_t &= \tanh(W_{xh} x_t + W_{hh} h_{t-1} + b_h) \\
o_t &= W_{ho} h_t + b_o
\end{aligned}
$$

- κ° μ΄μ‚° μ‹κ°„ λ‹¨κ³„ k μ—μ„ ν‘μ¤€ RNNμ€ μ΄μ „ λ‹¨κ³„μ hidden state $h_{k-1}$ μ™€ ν•¨κ» λ²΅ν„° $x_k$ λ¥Ό μ²λ¦¬ν•μ—¬ μ¶λ ¥ λ²΅ν„° $o_k$ λ¥Ό μƒμ„±ν•κ³  hidden stateλ¥Ό $h_k$λ΅ μ—…λ°μ΄νΈ
- hidden stateλ” λ„¤νΈμ›ν¬μ λ©”λ¨λ¦¬ μ—­ν• μ„ ν•λ©° κ³Όκ±° μ…λ ¥ μ— λ€ν• μ •λ³΄λ¥Ό μ μ§€.


| κΈ°νΈ | μλ―Έ |
|------|------|
| $x_t$ | μ…λ ¥ (at time t) |
| $h_t$ | hidden state |
| $o_t$ | μ¶λ ¥ |
| $W_{xh}$ | μ…λ ¥ β†’ hidden κ°€μ¤‘μΉ | λ¨λΈ μ…λ ¥μ„ hidden stateλ΅ μ²λ¦¬ν•λ” κ°€μ¤‘μΉ ν–‰λ ¬. 
| $W_{hh}$ | hidden β†’ hidden κ°€μ¤‘μΉ | hidden state κ°„μ λ°λ³µ μ—°κ²°
| $W_{oh}$ | hidden β†’ output κ°€μ¤‘μΉ | hidens stateμ—μ„ νμƒλ μ¶λ ¥μ„ μƒμ„±ν•λ”λ° μ‚¬μ©λλ” κ°€μ¤‘μΉ
| $b_h$, $b_o$ | νΈν–¥ (bias) |
| $tanh$ | ν™μ„±ν™” ν•¨μ (λΉ„μ„ ν•μ„± λ¶€μ—¬) |

## RNNμ ν•κ³„μ  
1.  long range dynamics λ¥Ό μ¶”μ¶ν•λ”λ° μ ν•μ  (κ°€μ¤‘μΉλ¥Ό λ°λ³µμ μΌλ΅ κ³±ν•λ©΄μ„ dilution or loss
2.  μμ°¨μ μΈ λ°μ΄ν„°λ¥Ό μ μ§„μ μΌλ΅ μ²λ¦¬ν•κΈ°μ— κ° μ‹κ°„ λ‹¨κ³„κ°€ μ΄μ „ μ‹κ°„ λ‹¨κ³„μ— μμ΅΄ ( κ³„μ‚° ν¨μ¨μ„± μ €ν• )

## π§® ν›λ ¨ κ°€λ¥ν• νλΌλ―Έν„°

| νλΌλ―Έν„° | ν¬κΈ° | μ„¤λ… |
|----------|------|------|
| $W_{xh}$ | $H \times D$ | μ…λ ¥ β†’ hidden λ³€ν™ |
| $W_{hh}$ | $H \times H$ | μν™ μ—°κ²° (μ΄μ „ hidden β†’ ν„μ¬ hidden) |
| $W_{ho}$ | $O \times H$ | hidden β†’ μ¶λ ¥ (optional) |
| $b_h$ | $H$ | hidden bias |
| $b_o$ | $O$ | output bias (optional) |

- $D$: μ…λ ¥ μ°¨μ›  
- $H$: hidden state μ°¨μ›  
- $O$: μ¶λ ¥ μ°¨μ›  

β… μ΄ λ¨λ“  νλΌλ―Έν„°λ” **trainable = True**λ΅ μ„¤μ •λμ–΄ ν•™μµλ¨

## β ν›λ ¨ λ¶κ°€λ¥ν• νλΌλ―Έν„° (Non-trainable)

| ν•­λ© | μ„¤λ… |
|------|------|
| ν•μ΄νΌνλΌλ―Έν„° | hidden size, learning rate, layer μ λ“± |
| μ΄κΈ° μƒνƒ $h_0$ | λ³΄ν†µ 0μΌλ΅ κ³ μ •λλ©° ν•™μµλμ§€ μ•μ |
| μ…λ ¥ λ°μ΄ν„° $x_t$ | ν•™μµμ λ€μƒμ΄ μ•„λ‹λΌ μ΅°κ±΄ |
| dropout λΉ„μ¨ λ“± | λ¨λΈ μ™Έλ¶€ μ„¤μ •κ°’ (ν•™μµλμ§€ μ•μ) |


## π” ν™μ„±ν™” ν•¨μ

- κΈ°λ³Έ RNNμ€ **$\tanh$** ν•¨μλ¥Ό μ‚¬μ©
- μ΄μ :  
  - μ¶λ ¥ λ²”μ„κ°€ $[-1, 1]$ β†’ **zero-centered**
  - ν•™μµ μ•μ •μ„± β†‘
- ReLUλ” μ μ‚¬μ©λμ§€ μ•μ (gradient vanish λ¬Έμ  μ μ§€λ§ memory μ μ§€μ— λ¶€μ ν•©)

## π§® μ—°μ‚° λ³µμ΅λ„ λ° λ³‘λ ¬μ„±

- RNNμ μ—°μ‚° λ³µμ΅λ„: $O(L D^2)$  
  - $L$: μ‹ν€€μ¤ κΈΈμ΄, $D$: hidden μ°¨μ› μ
- Transformer ($O(L^2 D)$)λ³΄λ‹¤ ν¨μ¨μ μΌ μ μμΌλ‚
- **μμ°¨μ μΈ κµ¬μ΅°**λ΅ μΈν•΄ λ³‘λ ¬ν™”κ°€ μ–΄λ ¤μ› **ν•™μµ μ†λ„ λ° ν¨μ¨μ„±μ€ λ‚®μ**

## β… νΉμ§• μ”μ•½

| ν•­λ© | μ„¤λ… |
|------|------|
| μ¥μ  | μ‹ν€€μ¤ κΈ°λ° μ…λ ¥ μ²λ¦¬, μ‹κ°„μ  λ§¥λ½ λ°μ |
| λ‹¨μ  | κΈ΄ μ‹ν€€μ¤μ—μ„ gradient vanishing/exploding λ¬Έμ  |
| μ‘μ© | μμ—°μ–΄ μ²λ¦¬, μμ„± μΈμ‹, μ‹κ³„μ—΄ μμΈ΅ λ“± |

